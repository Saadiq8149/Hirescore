# ðŸŽ“ HireScore

**Predicting Student Placement Success and Salary Outcomes using Machine Learning**

---

## ðŸ§­ About the Project

**HireScore** is a machine learning project that analyzes the academic and extracurricular profiles of engineering students to predict:
- âœ… **Placement status** (placed or not)
- ðŸ’° **Expected salary** (CTC in LPA)

This project was created as a milestone application of my learning from the **MIT 6.036: Introduction to Machine Learning** course, up to **Week 5 (Regression)**. The goal was not only to build something practically useful, but to deeply understand the ML workflow â€” from feature engineering and data preprocessing to model building and evaluation.

> ðŸ§  *This was my first complete end-to-end ML project â€” a hands-on, applied extension of what I learned in 6.036.*

---

## ðŸ” Problem Statement

Engineering students often wonder:
- *Will I get placed?*
- *What kind of salary offer can I expect?*

With increasing competition and uncertainty in campus placements, this project uses data to give students a sense of what factors influence outcomes, and what might be realistically expected.

---

## ðŸ“Š Dataset

- **Source**: [Kaggle â€“ Engineering Student Journey](https://www.kaggle.com/datasets/rakeshkapilavai/engineering-student-journey)
- **Records**: ~2000 students from various engineering branches
- **Features include**:
  - Semester-wise GPA (Sem1â€“Sem8)
  - Attendance %
  - Technical skills (count)
  - Club involvement (count)
  - Backlogs
  - Internship experience
  - Branch (CSE, ECE, IT, etc.)
  - Placement outcome and salary (CTC)

---

## ðŸ§  Models Used

### ðŸŸ© Classification â€“ Will the student be placed?
- Logistic Regression
- Random Forest Classifier

### ðŸŸ¦ Regression â€“ If placed, what salary (CTC) will they receive?
- Linear Regression
- Random Forest Regressor

---

## ðŸ“Œ Features Used for Modeling

| Category       | Features Used                                                   |
|----------------|------------------------------------------------------------------|
| **Academic**   | Average GPA, Sem6 GPA, GPA Trend, Backlogs                      |
| **Behavioral** | Attendance (%), Skill Count, Club Count                         |
| **Profile**    | Branch (one-hot encoded), Internship Done (Yes/No)              |

ðŸ“‰ Categorical data was one-hot encoded  
âš–ï¸ Continuous features were standardized  
ðŸ· Labels were encoded as 0/1 for classification and numeric for regression  

---

## ðŸ“ˆ Key Learnings

- ðŸ“¦ Hands-on implementation of concepts from Weeks 1â€“5 of MIT 6.036
- ðŸ§¹ Extensive data cleaning, feature engineering, and preprocessing
- ðŸ” Learned the importance of model selection (RF > Logistic in this case)
- ðŸ“Š Visualized feature importance to interpret model decisions
- ðŸš§ Faced and overcame challenges with noisy data and low signal-to-noise ratio in regression

---

## ðŸ™Œ Acknowledgements

- ðŸ“˜ MIT 6.036: *Introduction to Machine Learning* â€” for the concepts & intuition
- ðŸ“Š Dataset by Rakesh Kapilavayi on Kaggle
- ðŸ¤ Open-source Python ML stack: `pandas`, `scikit-learn`, `matplotlib`, `gradio`

---

> ðŸ”– **This was my first serious dive into applied ML** â€” not just learning algorithms, but using them to tell a story and solve a real-world problem. Onward and upward ðŸš€
